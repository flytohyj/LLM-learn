{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /data/social/llm/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/data/miniconda3/envs/py311/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:23<00:00,  3.37s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 748986.83 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 438357.62 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 680191.77 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:01<00:00, 82579.01 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 2432.37 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 3140.46 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  0%|                                                  | 0/3000 [00:00<?, ?it/s]/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.4428, 'grad_norm': 3.99804425239563, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9164, 'grad_norm': 3.50154447555542, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6883, 'grad_norm': 4.450908660888672, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.7645, 'grad_norm': 4.949592113494873, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 4.3029, 'grad_norm': 4.866971015930176, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 4.2062, 'grad_norm': 6.06423807144165, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 3.815, 'grad_norm': 6.418292999267578, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8162, 'grad_norm': 5.997838020324707, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5297, 'grad_norm': 5.106698989868164, 'learning_rate': 4.85e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0059, 'grad_norm': 5.531982421875, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7566, 'grad_norm': 5.113358974456787, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0254, 'grad_norm': 5.283050537109375, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8439, 'grad_norm': 5.428299903869629, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6738, 'grad_norm': 7.513107776641846, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4617, 'grad_norm': 6.644026756286621, 'learning_rate': 4.75e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6654, 'grad_norm': 6.091475963592529, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6412, 'grad_norm': 7.1824116706848145, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9492, 'grad_norm': 6.3500213623046875, 'learning_rate': 4.7e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7193, 'grad_norm': 6.101325988769531, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6914, 'grad_norm': 6.536591053009033, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3531, 'grad_norm': 6.75883150100708, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7412, 'grad_norm': 6.942580223083496, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5303, 'grad_norm': 6.739163875579834, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7602, 'grad_norm': 7.034115314483643, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5332, 'grad_norm': 6.750492095947266, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5775, 'grad_norm': 8.908007621765137, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.577, 'grad_norm': 7.868281364440918, 'learning_rate': 4.55e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5693, 'grad_norm': 8.513580322265625, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.548, 'grad_norm': 7.506573677062988, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6311, 'grad_norm': 7.875692367553711, 'learning_rate': 4.5e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6379, 'grad_norm': 8.02280044555664, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4402, 'grad_norm': 7.247089862823486, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3672, 'grad_norm': 8.654586791992188, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5365, 'grad_norm': 9.975455284118652, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4359, 'grad_norm': 7.578203201293945, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4643, 'grad_norm': 7.374202251434326, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5406, 'grad_norm': 7.758181571960449, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5922, 'grad_norm': 9.218621253967285, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5977, 'grad_norm': 7.823282718658447, 'learning_rate': 4.35e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3697, 'grad_norm': 7.942904472351074, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2699, 'grad_norm': 8.070550918579102, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6725, 'grad_norm': 10.8740816116333, 'learning_rate': 4.3e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5178, 'grad_norm': 9.862872123718262, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2834, 'grad_norm': 8.49086856842041, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7066, 'grad_norm': 9.417458534240723, 'learning_rate': 4.25e-05, 'epoch': 0.0}\n",
      "{'loss': 3.802, 'grad_norm': 9.795997619628906, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8355, 'grad_norm': 13.496346473693848, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2893, 'grad_norm': 10.408055305480957, 'learning_rate': 4.2e-05, 'epoch': 0.0}\n",
      "{'loss': 3.375, 'grad_norm': 7.908281326293945, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4217, 'grad_norm': 11.61683464050293, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.0}\n",
      " 17%|██████▋                                 | 500/3000 [00:59<05:23,  7.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.49s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.89s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.53s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.425 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 29.913214000000004, 'eval_rouge-2': 5.964978000000001, 'eval_rouge-l': 24.256868, 'eval_bleu-4': 0.02953163060522919, 'eval_runtime': 16.2217, 'eval_samples_per_second': 3.082, 'eval_steps_per_second': 0.247, 'epoch': 0.0}\n",
      " 17%|██████▋                                 | 500/3000 [01:15<05:23,  7.72it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.53s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-500\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4771, 'grad_norm': 10.237680435180664, 'learning_rate': 4.15e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4598, 'grad_norm': 8.773754119873047, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6371, 'grad_norm': 9.177739143371582, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4641, 'grad_norm': 9.372306823730469, 'learning_rate': 4.1e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4467, 'grad_norm': 9.602228164672852, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.602, 'grad_norm': 9.025936126708984, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5445, 'grad_norm': 10.339258193969727, 'learning_rate': 4.05e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4463, 'grad_norm': 11.413759231567383, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7523, 'grad_norm': 12.472776412963867, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.29, 'grad_norm': 7.282060623168945, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4145, 'grad_norm': 10.722821235656738, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6139, 'grad_norm': 9.5614652633667, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6496, 'grad_norm': 9.97059154510498, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.592, 'grad_norm': 8.404838562011719, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3391, 'grad_norm': 9.591415405273438, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4787, 'grad_norm': 9.146689414978027, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4936, 'grad_norm': 8.593975067138672, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4535, 'grad_norm': 7.816327095031738, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4584, 'grad_norm': 11.41297721862793, 'learning_rate': 3.85e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2652, 'grad_norm': 12.462651252746582, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5398, 'grad_norm': 8.56589412689209, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4025, 'grad_norm': 8.991305351257324, 'learning_rate': 3.8e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4521, 'grad_norm': 8.701576232910156, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5174, 'grad_norm': 11.241583824157715, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.217, 'grad_norm': 10.789633750915527, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4391, 'grad_norm': 10.365473747253418, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4586, 'grad_norm': 11.064496040344238, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3926, 'grad_norm': 10.215184211730957, 'learning_rate': 3.7e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4539, 'grad_norm': 8.900683403015137, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3375, 'grad_norm': 9.154784202575684, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3297, 'grad_norm': 9.873434066772461, 'learning_rate': 3.65e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4035, 'grad_norm': 9.663247108459473, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5516, 'grad_norm': 10.769804000854492, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4121, 'grad_norm': 9.076905250549316, 'learning_rate': 3.6e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3986, 'grad_norm': 8.6410493850708, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7295, 'grad_norm': 9.31898021697998, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3963, 'grad_norm': 10.841341972351074, 'learning_rate': 3.55e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4221, 'grad_norm': 9.231627464294434, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7006, 'grad_norm': 10.263749122619629, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3695, 'grad_norm': 7.789331912994385, 'learning_rate': 3.5e-05, 'epoch': 0.01}\n",
      "{'loss': 3.308, 'grad_norm': 8.812097549438477, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2633, 'grad_norm': 9.910804748535156, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5148, 'grad_norm': 11.175891876220703, 'learning_rate': 3.45e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2244, 'grad_norm': 9.474020957946777, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2023, 'grad_norm': 8.182052612304688, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4578, 'grad_norm': 8.706643104553223, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1938, 'grad_norm': 8.751721382141113, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4043, 'grad_norm': 8.524635314941406, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3195, 'grad_norm': 8.421807289123535, 'learning_rate': 3.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4998, 'grad_norm': 10.315488815307617, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\n",
      " 33%|█████████████                          | 1000/3000 [02:16<04:00,  8.31it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.51s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:17<00:06,  6.23s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.507036000000003, 'eval_rouge-2': 6.380649999999999, 'eval_rouge-l': 23.516976, 'eval_bleu-4': 0.03279232223345074, 'eval_runtime': 33.5694, 'eval_samples_per_second': 1.489, 'eval_steps_per_second': 0.119, 'epoch': 0.01}\n",
      " 33%|█████████████                          | 1000/3000 [02:50<04:00,  8.31it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:24<00:00,  6.39s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1000\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3805, 'grad_norm': 9.434075355529785, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4934, 'grad_norm': 11.310604095458984, 'learning_rate': 3.3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6832, 'grad_norm': 11.192049980163574, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3975, 'grad_norm': 12.60338020324707, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2416, 'grad_norm': 9.301162719726562, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3803, 'grad_norm': 8.552818298339844, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5211, 'grad_norm': 8.829397201538086, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.567, 'grad_norm': 10.445963859558105, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4893, 'grad_norm': 10.598773956298828, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5264, 'grad_norm': 11.220559120178223, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4141, 'grad_norm': 10.490097045898438, 'learning_rate': 3.15e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3197, 'grad_norm': 11.060813903808594, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3545, 'grad_norm': 8.236393928527832, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.677, 'grad_norm': 13.516204833984375, 'learning_rate': 3.1e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4486, 'grad_norm': 12.220602989196777, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4004, 'grad_norm': 10.539012908935547, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3707, 'grad_norm': 11.284215927124023, 'learning_rate': 3.05e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2945, 'grad_norm': 8.817204475402832, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.01}\n",
      "{'loss': 3.552, 'grad_norm': 9.787017822265625, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3281, 'grad_norm': 10.010127067565918, 'learning_rate': 3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2488, 'grad_norm': 10.101150512695312, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4088, 'grad_norm': 9.093198776245117, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1926, 'grad_norm': 9.794086456298828, 'learning_rate': 2.95e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4051, 'grad_norm': 11.304610252380371, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.366, 'grad_norm': 10.57444953918457, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.515, 'grad_norm': 10.810275077819824, 'learning_rate': 2.9e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4152, 'grad_norm': 10.18107795715332, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5523, 'grad_norm': 11.004792213439941, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5131, 'grad_norm': 13.26305866241455, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2258, 'grad_norm': 12.592789649963379, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.0768, 'grad_norm': 9.980133056640625, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2586, 'grad_norm': 9.531936645507812, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3482, 'grad_norm': 10.713415145874023, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3457, 'grad_norm': 8.7318696975708, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6719, 'grad_norm': 13.16562557220459, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.0938, 'grad_norm': 9.182344436645508, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2586, 'grad_norm': 9.353608131408691, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5348, 'grad_norm': 10.93716812133789, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4404, 'grad_norm': 15.923282623291016, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2072, 'grad_norm': 10.608540534973145, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6287, 'grad_norm': 9.065088272094727, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3475, 'grad_norm': 9.460823059082031, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3891, 'grad_norm': 10.161384582519531, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4516, 'grad_norm': 10.26738166809082, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4797, 'grad_norm': 12.480932235717773, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1508, 'grad_norm': 11.238749504089355, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2535, 'grad_norm': 10.528895378112793, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.0535, 'grad_norm': 8.68517017364502, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3805, 'grad_norm': 7.982452392578125, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2854, 'grad_norm': 12.047041893005371, 'learning_rate': 2.5e-05, 'epoch': 0.01}\n",
      " 50%|███████████████████▌                   | 1500/3000 [03:50<02:45,  9.04it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.46s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:04,  4.45s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.249018, 'eval_rouge-2': 6.571433999999999, 'eval_rouge-l': 24.082844, 'eval_bleu-4': 0.030296620853212258, 'eval_runtime': 22.7855, 'eval_samples_per_second': 2.194, 'eval_steps_per_second': 0.176, 'epoch': 0.01}\n",
      " 50%|███████████████████▌                   | 1500/3000 [04:13<02:45,  9.04it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  3.53s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1500\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1500/special_tokens_map.json\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3541, 'grad_norm': 11.03438949584961, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.515, 'grad_norm': 9.30279541015625, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2809, 'grad_norm': 9.811397552490234, 'learning_rate': 2.45e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4264, 'grad_norm': 9.119132995605469, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6648, 'grad_norm': 10.945232391357422, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3293, 'grad_norm': 9.098655700683594, 'learning_rate': 2.4e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2818, 'grad_norm': 9.724985122680664, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3648, 'grad_norm': 10.495932579040527, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4191, 'grad_norm': 11.165643692016602, 'learning_rate': 2.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2832, 'grad_norm': 9.036636352539062, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6436, 'grad_norm': 17.137046813964844, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3902, 'grad_norm': 9.919898986816406, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6303, 'grad_norm': 14.603772163391113, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5965, 'grad_norm': 9.756242752075195, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.01}\n",
      "{'loss': 3.218, 'grad_norm': 9.494019508361816, 'learning_rate': 2.25e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2885, 'grad_norm': 10.170450210571289, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3441, 'grad_norm': 9.424795150756836, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.65, 'grad_norm': 9.29002857208252, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3914, 'grad_norm': 9.568866729736328, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4336, 'grad_norm': 11.26715087890625, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5195, 'grad_norm': 9.575434684753418, 'learning_rate': 2.15e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5787, 'grad_norm': 8.842252731323242, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.223, 'grad_norm': 12.01683521270752, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.227, 'grad_norm': 11.604172706604004, 'learning_rate': 2.1e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1688, 'grad_norm': 11.857443809509277, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4473, 'grad_norm': 10.267170906066895, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.0387, 'grad_norm': 11.003328323364258, 'learning_rate': 2.05e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2209, 'grad_norm': 10.093914031982422, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3643, 'grad_norm': 11.208264350891113, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5381, 'grad_norm': 10.23410415649414, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.291, 'grad_norm': 12.42876148223877, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1842, 'grad_norm': 13.517589569091797, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1305, 'grad_norm': 12.232245445251465, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3514, 'grad_norm': 12.1739501953125, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4877, 'grad_norm': 10.460840225219727, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4869, 'grad_norm': 13.595322608947754, 'learning_rate': 1.9e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2328, 'grad_norm': 10.105500221252441, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4521, 'grad_norm': 9.603775024414062, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3439, 'grad_norm': 11.956313133239746, 'learning_rate': 1.85e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1285, 'grad_norm': 8.935674667358398, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2256, 'grad_norm': 10.477109909057617, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5133, 'grad_norm': 11.00197696685791, 'learning_rate': 1.8e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1736, 'grad_norm': 10.355976104736328, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4547, 'grad_norm': 11.669343948364258, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 2.99, 'grad_norm': 11.248820304870605, 'learning_rate': 1.75e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4686, 'grad_norm': 14.111078262329102, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4365, 'grad_norm': 15.71601390838623, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6398, 'grad_norm': 11.49311637878418, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4467, 'grad_norm': 11.308114051818848, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.235, 'grad_norm': 10.321514129638672, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.02}\n",
      " 67%|██████████████████████████             | 2000/3000 [05:12<02:01,  8.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.54s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.94s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.846590000000006, 'eval_rouge-2': 6.8285, 'eval_rouge-l': 24.033581999999996, 'eval_bleu-4': 0.03167192306922897, 'eval_runtime': 16.4346, 'eval_samples_per_second': 3.042, 'eval_steps_per_second': 0.243, 'epoch': 0.02}\n",
      " 67%|██████████████████████████             | 2000/3000 [05:29<02:01,  8.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.84s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.1137, 'grad_norm': 10.234939575195312, 'learning_rate': 1.65e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2793, 'grad_norm': 12.101195335388184, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2512, 'grad_norm': 14.469482421875, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1389, 'grad_norm': 10.272502899169922, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2668, 'grad_norm': 11.343996047973633, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6016, 'grad_norm': 13.816479682922363, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3678, 'grad_norm': 10.845837593078613, 'learning_rate': 1.55e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4312, 'grad_norm': 11.11871337890625, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5191, 'grad_norm': 12.182756423950195, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5061, 'grad_norm': 13.698739051818848, 'learning_rate': 1.5e-05, 'epoch': 0.02}\n",
      "{'loss': 3.473, 'grad_norm': 9.591952323913574, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2514, 'grad_norm': 9.929515838623047, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2705, 'grad_norm': 9.489253044128418, 'learning_rate': 1.45e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3906, 'grad_norm': 10.38099193572998, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3623, 'grad_norm': 10.933830261230469, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3451, 'grad_norm': 9.869402885437012, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2752, 'grad_norm': 10.46356201171875, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.7023, 'grad_norm': 12.82988452911377, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2965, 'grad_norm': 10.0980806350708, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2334, 'grad_norm': 10.285382270812988, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5039, 'grad_norm': 11.411128044128418, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5531, 'grad_norm': 12.361080169677734, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4535, 'grad_norm': 11.029296875, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5482, 'grad_norm': 14.183342933654785, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4654, 'grad_norm': 10.177266120910645, 'learning_rate': 1.25e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2967, 'grad_norm': 11.816056251525879, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4162, 'grad_norm': 11.238385200500488, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3172, 'grad_norm': 11.74112606048584, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.152, 'grad_norm': 11.711285591125488, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2746, 'grad_norm': 10.9214506149292, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2473, 'grad_norm': 11.53848648071289, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.02}\n",
      "{'loss': 3.307, 'grad_norm': 11.986979484558105, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.116, 'grad_norm': 11.563969612121582, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.074, 'grad_norm': 9.337209701538086, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5246, 'grad_norm': 12.602622985839844, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3604, 'grad_norm': 13.788067817687988, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3295, 'grad_norm': 9.01962947845459, 'learning_rate': 1.05e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4318, 'grad_norm': 15.424787521362305, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3936, 'grad_norm': 12.29067325592041, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3332, 'grad_norm': 11.975951194763184, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
      "{'loss': 3.398, 'grad_norm': 11.872103691101074, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3381, 'grad_norm': 11.154765129089355, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1398, 'grad_norm': 10.410563468933105, 'learning_rate': 9.5e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3592, 'grad_norm': 12.483048439025879, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2799, 'grad_norm': 14.942312240600586, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1541, 'grad_norm': 10.421642303466797, 'learning_rate': 9e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0848, 'grad_norm': 10.338315963745117, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.7971, 'grad_norm': 10.688579559326172, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1943, 'grad_norm': 12.628745079040527, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3326, 'grad_norm': 12.961311340332031, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.02}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [06:30<00:54,  9.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.50s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:03,  3.54s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.824502000000006, 'eval_rouge-2': 6.792714000000001, 'eval_rouge-l': 24.406152000000002, 'eval_bleu-4': 0.03144189104155078, 'eval_runtime': 16.0484, 'eval_samples_per_second': 3.116, 'eval_steps_per_second': 0.249, 'epoch': 0.02}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [06:46<00:54,  9.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  2.86s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2500\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2500/special_tokens_map.json\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.34, 'grad_norm': 12.361660957336426, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3916, 'grad_norm': 11.498034477233887, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1842, 'grad_norm': 13.568732261657715, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1791, 'grad_norm': 13.188202857971191, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4867, 'grad_norm': 12.366351127624512, 'learning_rate': 7.5e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4539, 'grad_norm': 10.846924781799316, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3129, 'grad_norm': 9.462345123291016, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3953, 'grad_norm': 12.387797355651855, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4738, 'grad_norm': 12.25761890411377, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4963, 'grad_norm': 12.060408592224121, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2338, 'grad_norm': 12.150601387023926, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2947, 'grad_norm': 12.15147590637207, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4924, 'grad_norm': 12.030709266662598, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3629, 'grad_norm': 13.625332832336426, 'learning_rate': 6e-06, 'epoch': 0.02}\n",
      "{'loss': 3.5936, 'grad_norm': 10.132568359375, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3604, 'grad_norm': 12.491521835327148, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4465, 'grad_norm': 10.276541709899902, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2674, 'grad_norm': 9.767000198364258, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4623, 'grad_norm': 11.370884895324707, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1068, 'grad_norm': 9.499372482299805, 'learning_rate': 5e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0461, 'grad_norm': 12.82422161102295, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0559, 'grad_norm': 9.979917526245117, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2479, 'grad_norm': 11.41749095916748, 'learning_rate': 4.5e-06, 'epoch': 0.02}\n",
      "{'loss': 3.333, 'grad_norm': 10.867603302001953, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2064, 'grad_norm': 10.223371505737305, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3559, 'grad_norm': 12.869739532470703, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3383, 'grad_norm': 12.967718124389648, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1068, 'grad_norm': 15.229560852050781, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3293, 'grad_norm': 11.835447311401367, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2418, 'grad_norm': 11.819424629211426, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.02}\n",
      "{'loss': 3.5291, 'grad_norm': 12.84415054321289, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.3873, 'grad_norm': 12.031620979309082, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2715, 'grad_norm': 10.144440650939941, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1412, 'grad_norm': 12.34743595123291, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2879, 'grad_norm': 10.868148803710938, 'learning_rate': 2.5e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0734, 'grad_norm': 11.086304664611816, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.02}\n",
      "{'loss': 3.7209, 'grad_norm': 11.598552703857422, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.03}\n",
      "{'loss': 3.4801, 'grad_norm': 12.534089088439941, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.03}\n",
      "{'loss': 3.0557, 'grad_norm': 10.429607391357422, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.03}\n",
      "{'loss': 3.284, 'grad_norm': 12.040084838867188, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.03}\n",
      "{'loss': 3.2219, 'grad_norm': 10.242217063903809, 'learning_rate': 1.5e-06, 'epoch': 0.03}\n",
      "{'loss': 2.9941, 'grad_norm': 12.771031379699707, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.03}\n",
      "{'loss': 3.6422, 'grad_norm': 10.974528312683105, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.03}\n",
      "{'loss': 3.4365, 'grad_norm': 11.934054374694824, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.03}\n",
      "{'loss': 3.3559, 'grad_norm': 11.948040962219238, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.03}\n",
      "{'loss': 3.2764, 'grad_norm': 12.831912994384766, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.03}\n",
      "{'loss': 3.3229, 'grad_norm': 11.743517875671387, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.03}\n",
      "{'loss': 3.3145, 'grad_norm': 14.143428802490234, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.03}\n",
      "{'loss': 3.2877, 'grad_norm': 10.844110488891602, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.03}\n",
      "{'loss': 3.201, 'grad_norm': 10.935924530029297, 'learning_rate': 0.0, 'epoch': 0.03}\n",
      "100%|███████████████████████████████████████| 3000/3000 [07:46<00:00,  9.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.50s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:12<00:03,  3.89s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.109678000000002, 'eval_rouge-2': 6.348091999999999, 'eval_rouge-l': 25.024354, 'eval_bleu-4': 0.03396039942731171, 'eval_runtime': 16.9609, 'eval_samples_per_second': 2.948, 'eval_steps_per_second': 0.236, 'epoch': 0.03}\n",
      "100%|███████████████████████████████████████| 3000/3000 [08:03<00:00,  9.20it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  3.13s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3000\n",
      "/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data/chatGLM3/model/chatGLM-mode3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 483.3881, 'train_samples_per_second': 6.206, 'train_steps_per_second': 6.206, 'train_loss': 3.4301943359375, 'epoch': 0.03}\n",
      "100%|███████████████████████████████████████| 3000/3000 [08:03<00:00,  6.21it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [04:54<00:00,  4.39s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 /data/miniconda3/envs/py311/bin/python3 finetune_hf.py  data/AdvertiseGen_fix  /data/chatGLM3/model/chatGLM-mode3  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2000  checkpoint-3000\n",
      "checkpoint-1500  checkpoint-2500  checkpoint-500\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:17<00:00,  2.53s/it]\n",
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (embedding): Embedding(\n",
      "      (word_embeddings): Embedding(65024, 4096)\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "    (encoder): GLMTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x GLMBlock(\n",
      "          (input_layernorm): RMSNorm()\n",
      "          (self_attention): SelfAttention(\n",
      "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
      "            (core_attention): CoreAttention(\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (post_attention_layernorm): RMSNorm()\n",
      "          (mlp): MLP(\n",
      "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layernorm): RMSNorm()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "  )\n",
      ")\n",
      "ChatGLMTokenizer(name_or_path='/data/chatGLM3/model/chatGLM-mode3', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t\n",
      "}\n",
      "这是一款性感的网纱百褶连衣裙，裙下摆采用了压褶设计，使得裙摆更加飘逸。整体风格为性感，适合喜欢展现女性魅力的你。裙衣门襟为拉链设计，方便穿脱。连衣裙采用了拼接和木耳边等细节设计，增加了整体的层次感和美感。拉链的设计使得裙款式更加丰富，同时抽褶和木耳边等设计元素也使得这款连衣裙更加有特色。不规则的裙长设计，为这款连衣裙增添了时尚感和个性。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  /data/miniconda3/envs/py311/bin/python3 inference_hf.py /data/chatGLM3/model/chatGLM-mode3/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f99166a-d399-4b1b-8d4b-eab5f59d0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "/data/social/llm/ChatGLM3/finetune_demo/output/checkpoint-3000\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:17<00:00,  2.49s/it]\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/social/llm/ChatGLM3/finetune_demo/\u001b[0m\u001b[1;33minference_hf.py\u001b[0m:\u001b[94m54\u001b[0m in \u001b[92mmain\u001b[0m           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_dir: Annotated[\u001b[96mstr\u001b[0m, typer.Argument(help=\u001b[33m'\u001b[0m\u001b[33m'\u001b[0m)],             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   \u001b[0mprompt: Annotated[\u001b[96mstr\u001b[0m, typer.Option(help=\u001b[33m'\u001b[0m\u001b[33m'\u001b[0m)],                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m):                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m54 \u001b[2m│   \u001b[0mmodel, tokenizer = load_model_and_tokenizer(model_dir)              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   \u001b[0mresponse, _ = model.chat(tokenizer, prompt)                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(response)                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/social/llm/ChatGLM3/finetune_demo/\u001b[0m\u001b[1;33minference_hf.py\u001b[0m:\u001b[94m32\u001b[0m in                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92mload_model_and_tokenizer\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m (model_dir / \u001b[33m'\u001b[0m\u001b[33madapter_config.json\u001b[0m\u001b[33m'\u001b[0m).exists():                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[94m1\u001b[0m)                                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(model_dir)                                                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m32 \u001b[2m│   │   \u001b[0mmodel = AutoPeftModelForCausalLM.from_pretrained(               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_dir, trust_remote_code=\u001b[94mTrue\u001b[0m, device_map=\u001b[33m'\u001b[0m\u001b[33mauto\u001b[0m\u001b[33m'\u001b[0m        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer_dir = model.peft_config[\u001b[33m'\u001b[0m\u001b[33mdefault\u001b[0m\u001b[33m'\u001b[0m].base_model_name_or \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/peft/\u001b[0m\u001b[1;33mauto.py\u001b[0m:\u001b[94m123\u001b[0m in \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m tokenizer_exists:                                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   │   │   \u001b[0mtokenizer = AutoTokenizer.from_pretrained(                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, trust_remote_code=kwarg \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   │   \u001b[0mbase_model.resize_token_embeddings(\u001b[96mlen\u001b[0m(tokenizer))         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/models\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/auto/\u001b[0m\u001b[1;33mtokenization_auto.py\u001b[0m:\u001b[94m822\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m819 \u001b[0m\u001b[2m│   │   │   \u001b[0m_ = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mcode_revision\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m820 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m os.path.isdir(pretrained_model_name_or_path):           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m821 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtokenizer_class.register_for_auto_class()              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m822 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m tokenizer_class.from_pretrained(                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *inputs, trust_remote_c \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m824 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m825 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m config_tokenizer_class \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_utils_base.py\u001b[0m:\u001b[94m2086\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2083 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2084 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mloading file \u001b[0m\u001b[33m{\u001b[0mfile_path\u001b[33m}\u001b[0m\u001b[33m from cache at \u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2085 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2086 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m._from_pretrained(                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2087 \u001b[0m\u001b[2m│   │   │   \u001b[0mresolved_vocab_files,                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2088 \u001b[0m\u001b[2m│   │   │   \u001b[0mpretrained_model_name_or_path,                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2089 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_configuration,                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_utils_base.py\u001b[0m:\u001b[94m2325\u001b[0m in \u001b[92m_from_pretrained\u001b[0m                                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2322 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2323 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Instantiate the tokenizer.\u001b[0m                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2324 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2325 \u001b[2m│   │   │   \u001b[0mtokenizer = \u001b[96mcls\u001b[0m(*init_inputs, **init_kwargs)              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2326 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mOSError\u001b[0m:                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2327 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mOSError\u001b[0m(                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2328 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUnable to load vocabulary from file. \u001b[0m\u001b[33m\"\u001b[0m               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/checkpoint-3000/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_chatglm.py\u001b[0m:\u001b[94m108\u001b[0m in \u001b[92m__init__\u001b[0m                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m<pad>\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m.tokenizer.pad_id                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.encode_special_tokens = encode_special_tokens             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m108 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(padding_side=padding_side, clean_up_tokenizat \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0mencode_special_tokens=encode_special_tokens,  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0m**kwargs)                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_utils.py\u001b[0m:\u001b[94m363\u001b[0m in \u001b[92m__init__\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 360 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._added_tokens_encoder: Dict[\u001b[96mstr\u001b[0m, \u001b[96mint\u001b[0m] = {k.content: v \u001b[94mfo\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 361 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 362 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# 4 init the parent class\u001b[0m                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 363 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(**kwargs)                                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 364 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# 4. If some of the special tokens are not part of the vocab,\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 366 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# the order of addition is the same as self.SPECIAL_TOKENS_AT\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_utils_base.py\u001b[0m:\u001b[94m1618\u001b[0m in \u001b[92m__init__\u001b[0m                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1615 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# we reconstruct that into a single dict while loading th\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1616 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.chat_template = {template[\u001b[33m\"\u001b[0m\u001b[33mname\u001b[0m\u001b[33m\"\u001b[0m]: template[\u001b[33m\"\u001b[0m\u001b[33mtemplat\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1617 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1618 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(**kwargs)                                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1619 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1620 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@property\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1621 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmax_len_single_sentence\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mint\u001b[0m:                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/data/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[1;33mtokeni\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mzation_utils_base.py\u001b[0m:\u001b[94m872\u001b[0m in \u001b[92m__init__\u001b[0m                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 869 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m), \u001b[33m\"\u001b[0m\u001b[33mOne of the tokens is not a string or an Added\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 870 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96msetattr\u001b[0m(\u001b[96mself\u001b[0m, key, value)                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 871 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(value, (\u001b[96mstr\u001b[0m, AddedToken)):            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 872 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96msetattr\u001b[0m(\u001b[96mself\u001b[0m, key, value)                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 873 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 874 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mSpecial token \u001b[0m\u001b[33m{\u001b[0mkey\u001b[33m}\u001b[0m\u001b[33m has to be e\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 875 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mAttributeError: \u001b[0mproperty \u001b[32m'eos_token'\u001b[0m of \u001b[32m'ChatGLMTokenizer'\u001b[0m object has no setter\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  /data/miniconda3/envs/py311/bin/python3 inference_hf.py /data/social/llm/ChatGLM3/finetune_demo/output/checkpoint-3000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162e171-8ebe-485d-92a4-6992cf1bf993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e34bb-749f-4e9f-a9aa-7fcad92f6e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
